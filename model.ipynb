{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33536a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4bdedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_to_label(proba_batch):\n",
    "    # Detach the tensor and convert it to a NumPy array\n",
    "    proba_batch_np = proba_batch.detach().numpy()\n",
    "\n",
    "    # Find the index of the largest value in each sub-array\n",
    "    max_indices = np.argmax(proba_batch_np, axis=1)\n",
    "\n",
    "    # Create a new array of the same shape filled with 0s\n",
    "    binary_array = np.zeros_like(proba_batch_np)\n",
    "\n",
    "    # Set the largest value positions to 1\n",
    "    for i, max_index in enumerate(max_indices):\n",
    "        binary_array[i, max_index] = 1\n",
    "\n",
    "    return binary_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a9083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build architecture\n",
    "\n",
    "# Distilled Dual-task Deep Averaging Net\n",
    "class DistilledDAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch implementation for Deep Averaging Network for classification \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes,\n",
    "                       embedding_dim: int, \n",
    "                       hidden_dim1: int, \n",
    "                       hidden_dim2: int, \n",
    "                       leaky_relu_negative_slope: float, \n",
    "                       dropout_probability: float\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create the network architecture. \n",
    "        In our sentiment analysis, we have three classes: 0, 1, 2\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.leaky_relu_negative_slope = leaky_relu_negative_slope\n",
    "        self.dropout_probability = dropout_probability\n",
    "        \n",
    "        self.hidden1 = nn.Linear(self.embedding_dim, self.hidden_dim1)\n",
    "        self.hidden2 = nn.Linear(self.hidden_dim1,self.hidden_dim2)\n",
    "        self.theta = nn.Linear(self.hidden_dim2, self.num_classes)\n",
    "        \n",
    "        self.log_softmax = nn.LogSoftmax(dim=1) # A dimension along which LogSoftmax will be computed.\n",
    "        self.apply_dropout = nn.Dropout(self.dropout_probability)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor containing embedded word vectors.\n",
    "                              Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Log probability of each class. Shape: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Average the input word embeddings\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Pass through the shared layers\n",
    "        x = self.hidden1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_negative_slope)\n",
    "        x = self.apply_dropout(x)\n",
    "\n",
    "        x = self.hidden2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_negative_slope)\n",
    "        x = self.apply_dropout(x)\n",
    "\n",
    "        # Pass through final layer\n",
    "        x = self.theta(x)\n",
    "\n",
    "        # Apply the LogSoftmax activation function\n",
    "        x = self.log_softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_model(self,\n",
    "                    X_train,\n",
    "                    Y_train,\n",
    "                    X_dev,\n",
    "                    Y_dev,\n",
    "                    soft_labels,\n",
    "                    optimizer,\n",
    "                    num_iterations,\n",
    "                    soft_label_weight=0.5,\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\n",
    "                    batch_size=500,\n",
    "                    check_every=10,\n",
    "                    verbose=False):\n",
    "        \"\"\"\n",
    "        Method to train the model. \n",
    "\n",
    "        soft_labels are only available for the training set. \n",
    "        \"\"\"\n",
    "\n",
    "        # Let the model know that we're in training mode, which is important for dropout\n",
    "        self.train()\n",
    "\n",
    "        loss_history = []\n",
    "        train_accuracy = []\n",
    "        dev_accuracy = []\n",
    "\n",
    "        for t in range(num_iterations):\n",
    "            if batch_size >= X_train.shape[0]: \n",
    "                X_batch = X_train\n",
    "                Y_batch = Y_train\n",
    "                soft_labels_batch = soft_labels\n",
    "            else:\n",
    "                batch_indices = np.random.randint(X_train.shape[0], size=batch_size)\n",
    "                X_batch = X_train[batch_indices]\n",
    "                Y_batch = Y_train[batch_indices]\n",
    "                soft_labels_batch = soft_labels[batch_indices]\n",
    "\n",
    "            # Forward pass \n",
    "            log_probs_batch = self.forward(X_batch)\n",
    "\n",
    "            # Distillation loss (cross entropy loss with hard labels + cross entropy loss with soft labels)\n",
    "            # weighted with soft and hard label\n",
    "            loss = (1 - soft_label_weight) * loss_fn(log_probs_batch, Y_batch) + \\\n",
    "                    soft_label_weight * loss_fn(log_probs_batch, soft_labels_batch)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % check_every == 0:\n",
    "                loss_value = loss.item()\n",
    "                loss_history.append(loss_value)\n",
    "\n",
    "                # Check train accuracy (entire set, not just batch) \n",
    "                train_y_pred = self.predict(X_train)\n",
    "                train_acc = self.accuracy(train_y_pred, Y_train.detach().numpy()) \n",
    "                train_accuracy.append(train_acc)\n",
    "\n",
    "                # Check dev accuracy (entire set, not just batch) \n",
    "                dev_y_pred = self.predict(X_dev)\n",
    "                dev_acc = self.accuracy(dev_y_pred, Y_dev.detach().numpy())\n",
    "                dev_accuracy.append(dev_acc)\n",
    "\n",
    "                if verbose: print(f\"Iteration={t}, Loss={loss_value}\")\n",
    "\n",
    "        return loss_history, train_accuracy, dev_accuracy\n",
    "\n",
    "    \n",
    "    def predict(self, X, proba_mode=False):\n",
    "        \"\"\"\n",
    "        Method to make predictions given a trained model. \n",
    "        \n",
    "        No need to modify this method. \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        log_probs_batch = self.forward(X)\n",
    "\n",
    "        if proba_mode:\n",
    "            return log_probs_batch\n",
    "        else:\n",
    "            # Convert log probabilities to labels\n",
    "            label_batch = proba_to_label(log_probs_batch)\n",
    "            return label_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred: np.ndarray, y_true: np.ndarray) -> float: \n",
    "        \"\"\"\n",
    "        Calculates accuracy. No need to modify this method. \n",
    "        \"\"\"\n",
    "        return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c140f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
