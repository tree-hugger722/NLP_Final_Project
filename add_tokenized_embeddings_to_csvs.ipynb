{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e263e0b1",
   "metadata": {},
   "source": [
    "### Add Tokenized Text and Embeddings to CSVS\n",
    "Run all of the following cells, in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dcf1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "imLoad DatasetLoad Datasetport timeLoad Dataset\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5ee47",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30b5d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"\n",
    "    Load training and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    train = pd.read_csv(\"./new_dataset/train_preprocessed.csv\")\n",
    "    dev = pd.read_csv(\"./new_dataset/val_preprocessed.csv\")\n",
    "    test = pd.read_csv(\"./new_dataset/test_preprocessed.csv\")\n",
    "    \n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce5576",
   "metadata": {},
   "source": [
    "Tokenizing Input Text and Adding to Train and Dev Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6677d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text: str):\n",
    "    '''\n",
    "    NLTK Tweet Tokenizer -- removes handles\n",
    "\n",
    "    @param text        string tweet\n",
    "    @ret tokens        list of tokens\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df731f",
   "metadata": {},
   "source": [
    "Embedding Input Text Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e32f6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Loads embeddings from embedding file and creates \n",
    "    1) dictionary of embedding words to indices\n",
    "    2) list of embedding indices to words\n",
    "    3) dense word embedding matrix\n",
    "    \"\"\"\n",
    "    embeddings = KeyedVectors.load_word2vec_format(filename, binary=False, no_header=True)\n",
    "    vocab2indx = dict(embeddings.key_to_index)\n",
    "    idx2vocab = list(embeddings.index_to_key)\n",
    "    embed_array = embeddings.vectors # matrix of dense word embeddings \n",
    "                                     # rows: a word \n",
    "                                     # columns: dimensions (50) of the dense embeddings\n",
    "    return vocab2indx, idx2vocab, embed_array\n",
    "\n",
    "\n",
    "def add_the_embedding(embed_array, vocab2indx): \n",
    "    \"\"\"\n",
    "    Adds \"the\" embedding to the embed_array matrix\n",
    "    \"\"\"\n",
    "    the_embedding = embed_array[vocab2indx[\"the\"]]\n",
    "    out = np.vstack((embed_array, the_embedding))\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_oov(idx2vocab, vocab2indx, embed_array):\n",
    "    \"\"\"\n",
    "    Adds <OOV> token to embedded vocabulary\n",
    "    \"\"\"\n",
    "    print(\"len embed array: \", len(embed_array))\n",
    "    new_oov_entry = len(embed_array)\n",
    "    idx2vocab += [\"<OOV>\"]\n",
    "    vocab2indx[\"<OOV>\"] = new_oov_entry\n",
    "    embed_array_w_oov = add_the_embedding(embed_array, vocab2indx)\n",
    "\n",
    "    return idx2vocab, vocab2indx, embed_array_w_oov\n",
    "\n",
    "\n",
    "def add_pad(idx2vocab, vocab2indx, embed_array):\n",
    "    \"\"\"\n",
    "    Adds <PAD> token to embedded vocabulary\n",
    "    \"\"\"\n",
    "    print(\"len embed array: \", len(embed_array))\n",
    "    new_pad_entry = len(embed_array)\n",
    "    idx2vocab += [\"<PAD>\"]\n",
    "    vocab2indx[\"<PAD>\"] = new_pad_entry\n",
    "    embed_array_w_pad = add_the_embedding(embed_array, vocab2indx)\n",
    "    \n",
    "    return idx2vocab, vocab2indx, embed_array_w_pad\n",
    "\n",
    "\n",
    "def truncate(original_indices_list: list, maximum_length=128) -> list: \n",
    "    \"\"\"\n",
    "    Truncates the original_indices_list to the maximum_length\n",
    "    \"\"\"\n",
    "    return original_indices_list[0:maximum_length]\n",
    "\n",
    "\n",
    "def pad(original_indices_list: list, pad_index: int, maximum_length=128) -> list: \n",
    "    \"\"\"\n",
    "    Given original_indices_list, concatenates the pad_index enough times \n",
    "    to make the list to maximum_length. \n",
    "    \"\"\"\n",
    "    while len(original_indices_list) < maximum_length:\n",
    "        original_indices_list.append(pad_index)\n",
    "        \n",
    "    return original_indices_list\n",
    "\n",
    "\n",
    "def get_padded_oov_embeddings():\n",
    "    \"\"\"\n",
    "    Get embedding array which includes the <PAD> and <OOV> tokens\n",
    "    \"\"\"\n",
    "    vocab2indx, idx2vocab, embed_array = load_embeddings(\"glove.twitter.27B.50d.txt\")\n",
    "    idx2vocab, vocab2indx, embed_array_w_oov = add_oov(idx2vocab, vocab2indx, embed_array)\n",
    "    idx2vocab, vocab2indx, embed_array_w_oov_pad = add_pad(idx2vocab, vocab2indx, embed_array_w_oov)\n",
    "    \n",
    "    return embed_array_w_oov_pad, vocab2indx, idx2vocab\n",
    "\n",
    "def create_word_indices(tokens, vocab2indx): \n",
    "    \"\"\"\n",
    "    For each example, translate each token into its corresponding index from vocab2indx\n",
    "    \n",
    "    Replace words not in the vocabulary with the symbol \"<OOV>\" \n",
    "        which stands for 'out of vocabulary'\n",
    "        \n",
    "    Arguments: \n",
    "       - tokens (List[str]): list of strings of tokens \n",
    "       - vocab2indx (dict): each vocabulary word as strings and its corresponding int index \n",
    "                           for the embeddings \n",
    "                           \n",
    "    Returns: \n",
    "        - (List[int]): list of integers\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    num_oov = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in vocab2indx:\n",
    "            token = \"<OOV>\"\n",
    "            num_oov += 1\n",
    "        indices.append(vocab2indx[token])\n",
    "    \n",
    "    return indices, num_oov, len(tokens)\n",
    "\n",
    "\n",
    "def convert_X(Xmat, embeddings, vocab2indx, idx2vocab):\n",
    "    MAXIMUM_LENGTH = 128\n",
    "    \n",
    "    X_list_embedded = []\n",
    "    X_list_tokenized = []\n",
    "    num_total_tokens = 0\n",
    "    num_oov = 0\n",
    "    \n",
    "    for one_example in Xmat:\n",
    "        one_example = str(one_example)\n",
    "        one_example_tokenized = tokenizer(one_example)\n",
    "        X_list_tokenized.append(one_example_tokenized)\n",
    "        example_indices, num_oov_in_example, num_tokens_in_example = create_word_indices(one_example, vocab2indx)\n",
    "        example_indices = truncate(example_indices, maximum_length=MAXIMUM_LENGTH)\n",
    "        example_indices = pad(example_indices, len(vocab2indx)-1, maximum_length=MAXIMUM_LENGTH)\n",
    "        \n",
    "        example_embeddings = [] # A list of token embeddings\n",
    "        \n",
    "        for index in example_indices:\n",
    "            example_embeddings.append(embeddings[index])\n",
    "        \n",
    "        X_list_embedded.append(example_embeddings)\n",
    "        \n",
    "        num_total_tokens += num_tokens_in_example\n",
    "        num_oov += num_oov_in_example\n",
    "        percent_oov = (num_oov/num_total_tokens)\n",
    "        \n",
    "    return X_list_tokenized, X_list_embedded, percent_oov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03910a2f",
   "metadata": {},
   "source": [
    "Adding Embeddings and Tokenized Text to Train and Test Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0af2781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train, dev, test = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd2f0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len embed array:  1193514\n",
      "len embed array:  1193515\n"
     ]
    }
   ],
   "source": [
    "# Get GloVE embeddings\n",
    "embeddings, vocab2indx, idx2vocab = get_padded_oov_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "550354bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_tokenized_embeddings_to_csvs():\n",
    "    '''\n",
    "    Converts twitter data into arrays of GloVE embeddings for each dataset\n",
    "    and adds them to each DataFrame\n",
    "\n",
    "    @ret       updated train, dev, and test datasets as Pandas DataFrames\n",
    "    '''\n",
    "    # Convert twitter data into arrays of GloVE embeddings for each dataset\n",
    "    X_train_tokens, X_train_embedded, percent_train_oov = convert_X(train[\"text\"], embeddings, vocab2indx, idx2vocab)\n",
    "    X_dev_tokens, X_dev_embedded, percent_dev_oov = convert_X(dev[\"text\"], embeddings, vocab2indx, idx2vocab)\n",
    "    X_test_tokens, X_test_embedded, percent_test_oov = convert_X(test[\"text\"], embeddings, vocab2indx, idx2vocab)\n",
    "\n",
    "    # Add 'Embedding' and 'Tokenized Text' Columns to Dataframes\n",
    "    train[\"Embedding\"] = X_train_embedded\n",
    "    train[\"Tokenized Text\"] = X_train_tokens\n",
    "    dev[\"Embedding\"] = X_dev_embedded\n",
    "    dev[\"Tokenized Text\"] = X_dev_tokens\n",
    "    test[\"Embedding\"] = X_test_embedded\n",
    "    test[\"Tokenized Text\"] = X_test_tokens\n",
    "\n",
    "    print(\"Percentage of train tokens out-of-vocabulary: \", percent_train_oov)\n",
    "    print(\"Percentage of dev tokens out-of-vocabulary: \", percent_dev_oov)\n",
    "    print(\"Percentage of test tokens out-of-vocabulary: \", percent_test_oov)\n",
    "    \n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff03db90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of train tokens out-of-vocabulary:  0.2545306824346183\n",
      "Percentage of dev tokens out-of-vocabulary:  0.2516598166297819\n",
      "Percentage of test tokens out-of-vocabulary:  0.23862955833600685\n"
     ]
    }
   ],
   "source": [
    "# Update train, dev, and test sets with tokenized text and embeddings\n",
    "train, dev, test = add_tokenized_embeddings_to_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f47d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check: Do the dataframes now contain 'Embedding' and 'Tokenized Text' columns?\n",
    "print(train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20673383",
   "metadata": {},
   "source": [
    "Adding Embeddings and Tokenized Text to Train and Test CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"new_dataset/train_final.csv\")\n",
    "dev.to_csv(\"new_dataset/val_final.csv\")\n",
    "test.to_csv(\"new_dataset/test_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cs375] *",
   "language": "python",
   "name": "conda-env-.conda-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
