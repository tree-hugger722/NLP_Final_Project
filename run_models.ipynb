{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e263e0b1",
   "metadata": {},
   "source": [
    "### Add Tokenized Text and Embeddings to CSVS\n",
    "Run all of the following cells, in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4a2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ast\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "import torch\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5ee47",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c784db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"\n",
    "    Load training and dev sets\n",
    "    \"\"\"\n",
    "\n",
    "    train = pd.read_csv(\"./new_dataset/train_preprocessed.csv\")\n",
    "    dev = pd.read_csv(\"./new_dataset/val_preprocessed.csv\")\n",
    "    \n",
    "    return train, dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce5576",
   "metadata": {},
   "source": [
    "Tokenizing Input Text and Adding to Train and Dev Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa2742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text: str):\n",
    "    '''\n",
    "    NLTK Tweet Tokenizer -- removes handles\n",
    "\n",
    "    @param text        string tweet\n",
    "    @ret tokens        list of tokens\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df731f",
   "metadata": {},
   "source": [
    "Embedding Input Text Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ad0c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Loads embeddings from embedding file and creates \n",
    "    1) dictionary of embedding words to indices\n",
    "    2) list of embedding indices to words\n",
    "    3) dense word embedding matrix\n",
    "    \"\"\"\n",
    "    embeddings = KeyedVectors.load_word2vec_format(filename, binary=False, no_header=True)\n",
    "    vocab2indx = dict(embeddings.key_to_index)\n",
    "    idx2vocab = list(embeddings.index_to_key)\n",
    "    embed_array = embeddings.vectors # matrix of dense word embeddings \n",
    "                                     # rows: a word \n",
    "                                     # columns: dimensions (50) of the dense embeddings\n",
    "    return vocab2indx, idx2vocab, embed_array\n",
    "\n",
    "\n",
    "def add_the_embedding(embed_array, vocab2indx): \n",
    "    \"\"\"\n",
    "    Adds \"the\" embedding to the embed_array matrix\n",
    "    \"\"\"\n",
    "    the_embedding = embed_array[vocab2indx[\"the\"]]\n",
    "    out = np.vstack((embed_array, the_embedding))\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_oov(idx2vocab, vocab2indx, embed_array):\n",
    "    \"\"\"\n",
    "    Adds <OOV> token to embedded vocabulary\n",
    "    \"\"\"\n",
    "    print(\"len embed array: \", len(embed_array))\n",
    "    new_oov_entry = len(embed_array)\n",
    "    idx2vocab += [\"<OOV>\"]\n",
    "    vocab2indx[\"<OOV>\"] = new_oov_entry\n",
    "    embed_array_w_oov = add_the_embedding(embed_array, vocab2indx)\n",
    "\n",
    "    return idx2vocab, vocab2indx, embed_array_w_oov\n",
    "\n",
    "\n",
    "def add_pad(idx2vocab, vocab2indx, embed_array):\n",
    "    \"\"\"\n",
    "    Adds <PAD> token to embedded vocabulary\n",
    "    \"\"\"\n",
    "    print(\"len embed array: \", len(embed_array))\n",
    "    new_pad_entry = len(embed_array)\n",
    "    idx2vocab += [\"<PAD>\"]\n",
    "    vocab2indx[\"<PAD>\"] = new_pad_entry\n",
    "    embed_array_w_pad = add_the_embedding(embed_array, vocab2indx)\n",
    "    \n",
    "    return idx2vocab, vocab2indx, embed_array_w_pad\n",
    "\n",
    "\n",
    "def truncate(original_indices_list: list, maximum_length=128) -> list: \n",
    "    \"\"\"\n",
    "    Truncates the original_indices_list to the maximum_length\n",
    "    \"\"\"\n",
    "    return original_indices_list[0:maximum_length]\n",
    "\n",
    "\n",
    "def pad(original_indices_list: list, pad_index: int, maximum_length=128) -> list: \n",
    "    \"\"\"\n",
    "    Given original_indices_list, concatenates the pad_index enough times \n",
    "    to make the list to maximum_length. \n",
    "    \"\"\"\n",
    "    while len(original_indices_list) < maximum_length:\n",
    "        original_indices_list.append(pad_index)\n",
    "        \n",
    "    return original_indices_list\n",
    "\n",
    "\n",
    "def get_padded_oov_embeddings():\n",
    "    \"\"\"\n",
    "    Get embedding array which includes the <PAD> and <OOV> tokens\n",
    "    \"\"\"\n",
    "    vocab2indx, idx2vocab, embed_array = load_embeddings(\"glove.twitter.27B.50d.txt\")\n",
    "    idx2vocab, vocab2indx, embed_array_w_oov = add_oov(idx2vocab, vocab2indx, embed_array)\n",
    "    idx2vocab, vocab2indx, embed_array_w_oov_pad = add_pad(idx2vocab, vocab2indx, embed_array_w_oov)\n",
    "    \n",
    "    return embed_array_w_oov_pad, vocab2indx, idx2vocab\n",
    "\n",
    "def create_word_indices(tokens, vocab2indx): \n",
    "    \"\"\"\n",
    "    For each example, translate each token into its corresponding index from vocab2indx\n",
    "    \n",
    "    Replace words not in the vocabulary with the symbol \"<OOV>\" \n",
    "        which stands for 'out of vocabulary'\n",
    "        \n",
    "    Arguments: \n",
    "       - tokens (List[str]): list of strings of tokens \n",
    "       - vocab2indx (dict): each vocabulary word as strings and its corresponding int index \n",
    "                           for the embeddings \n",
    "                           \n",
    "    Returns: \n",
    "        - (List[int]): list of integers\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    num_oov = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in vocab2indx:\n",
    "            token = \"<OOV>\"\n",
    "            num_oov += 1\n",
    "        indices.append(vocab2indx[token])\n",
    "    \n",
    "    return indices, num_oov, len(tokens)\n",
    "\n",
    "\n",
    "def convert_X(Xmat, embeddings, vocab2indx, idx2vocab):\n",
    "    MAXIMUM_LENGTH = 128\n",
    "    \n",
    "    X_list_embedded = []\n",
    "    num_total_tokens = 0\n",
    "    num_oov = 0\n",
    "    \n",
    "    for one_example in Xmat:\n",
    "        one_example = str(one_example)\n",
    "        one_example_tokenized = tokenizer(one_example)\n",
    "        example_indices, num_oov_in_example, num_tokens_in_example = create_word_indices(one_example_tokenized, vocab2indx)\n",
    "        example_indices = truncate(example_indices, maximum_length=MAXIMUM_LENGTH)\n",
    "        example_indices = pad(example_indices, len(vocab2indx)-1, maximum_length=MAXIMUM_LENGTH)\n",
    "        \n",
    "        example_embeddings = [] # A list of token embeddings\n",
    "        \n",
    "        for index in example_indices:\n",
    "            example_embeddings.append(embeddings[index])\n",
    "        \n",
    "        X_list_embedded.append(example_embeddings)\n",
    "        \n",
    "        num_total_tokens += num_tokens_in_example\n",
    "        num_oov += num_oov_in_example\n",
    "        percent_oov = (num_oov/num_total_tokens)\n",
    "        \n",
    "    return torch.FloatTensor(X_list_embedded), percent_oov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03910a2f",
   "metadata": {},
   "source": [
    "Get embedded input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa085621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train, dev = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0073b71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len embed array:  1193514\n",
      "len embed array:  1193515\n"
     ]
    }
   ],
   "source": [
    "# Get GloVE embeddings\n",
    "embeddings, vocab2indx, idx2vocab = get_padded_oov_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7403464a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/s6xvfp8x29j9m9pl6w7jn8hh0000gn/T/ipykernel_21663/498052130.py:132: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_2a19nf9hj1/croot/pytorch_1675190251927/work/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  return torch.FloatTensor(X_list_embedded), percent_oov\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of train tokens out-of-vocabulary:  0.08425232669426272\n",
      "Percentage of dev tokens out-of-vocabulary:  0.08289588312819711\n"
     ]
    }
   ],
   "source": [
    "# Convert train and dev sets to GloVE embeddings\n",
    "# X_train shape: (num_examples_test, 64, 50)\n",
    "# X_dev shape: (num_examples_test, 64, 50)\n",
    "Xmat_train, percent_train_oov = convert_X(train[\"text\"], embeddings, vocab2indx, idx2vocab)\n",
    "Xmat_dev, percent_dev_oov = convert_X(dev[\"text\"], embeddings, vocab2indx, idx2vocab)\n",
    "\n",
    "print(\"Percentage of train tokens out-of-vocabulary: \", percent_train_oov)\n",
    "print(\"Percentage of dev tokens out-of-vocabulary: \", percent_dev_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c09a0b",
   "metadata": {},
   "source": [
    "Run Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc462a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_embedding(df):\n",
    "    \"\"\"\n",
    "    Convert a tensor of shape (batch_size, num_sentences, embedding_size) to\n",
    "    (batch_size, embedding_size) by averaging the embeddings along the second dimension.\n",
    "\n",
    "    :param df: Input tensor with shape (batch_size, num_sentences, embedding_size)\n",
    "    :type df: torch.Tensor\n",
    "    :return: Averaged tensor with shape (batch_size, embedding_size)\n",
    "    :rtype: torch.Tensor\n",
    "    \"\"\"\n",
    "    # Check if the input is a PyTorch tensor\n",
    "    if not isinstance(df, torch.Tensor):\n",
    "        raise TypeError(\"Input must be a PyTorch tensor.\")\n",
    "\n",
    "    # Check if the input tensor has the correct shape\n",
    "    if len(df.shape) != 3:\n",
    "        raise ValueError(\"Input tensor must have 3 dimensions (batch_size, num_sentences, embedding_size).\")\n",
    "\n",
    "    # Compute the average along the second dimension (num_sentences)\n",
    "    averaged_embeddings = df.mean(dim=1)\n",
    "\n",
    "    return averaged_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef00693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline train accuracy: 0.6146443056012276\n",
      "Baseline dev accuracy: 0.612\n"
     ]
    }
   ],
   "source": [
    "# Average the word embeddings for each example into a sentence embedding\n",
    "X_train_avg = average_sentence_embedding(Xmat_train) # shape: (num_examples_test, 50)\n",
    "X_dev_avg = average_sentence_embedding(Xmat_dev) # shape: (num_examples_test, 50)\n",
    "\n",
    "# Get Y\n",
    "Y_train = train[\"label\"]\n",
    "Y_dev = dev[\"label\"]\n",
    "\n",
    "# Train a Logistic Regression model using the averaged embeddings\n",
    "baseline_embed = LogisticRegression(max_iter=10000, multi_class='auto', solver='lbfgs')\n",
    "baseline_embed.fit(X_train_avg, Y_train)\n",
    "\n",
    "# Prediction & Evaluation\n",
    "Y_pred_train = baseline_embed.predict(X_train_avg)\n",
    "Y_pred_dev = baseline_embed.predict(X_dev_avg)\n",
    "\n",
    "train_accuracy = accuracy_score(Y_train, Y_pred_train)\n",
    "dev_accuracy = accuracy_score(Y_dev, Y_pred_dev)\n",
    "\n",
    "print(f\"Baseline train accuracy: {train_accuracy}\")\n",
    "print(f\"Baseline dev accuracy: {dev_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc5a71",
   "metadata": {},
   "source": [
    "### Distilled Deep Averaging Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb05359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499f85a",
   "metadata": {},
   "source": [
    "Helper function to convert an array of log probabilities to a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f9e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_to_label(proba_batch):\n",
    "    '''\n",
    "    Given a proba_batch, an array of probabilities over each sentiment class (0, 1, 2), returns\n",
    "    a single label corresponding to the maximum probability\n",
    "    '''\n",
    "    \n",
    "    # Detach the tensor and convert it to a NumPy array\n",
    "    proba_batch_np = proba_batch.detach().numpy()\n",
    "\n",
    "    # Find the index of the largest value in each sub-array\n",
    "    max_indices = np.argmax(proba_batch_np, axis=1)\n",
    "\n",
    "#     # Create a new array of the same shape filled with 0s\n",
    "#     binary_array = np.zeros_like(proba_batch_np)\n",
    "\n",
    "#     # Set the largest value positions to 1\n",
    "#     for i, max_index in enumerate(max_indices):\n",
    "#         binary_array[i, max_index] = 1\n",
    "\n",
    "    return max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "158edc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get soft labels\n",
    "Y_soft_train = train[\"Y_soft\"]\n",
    "Y_soft_train = np.array([literal_eval(row) for row in Y_soft_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5f8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        2\n",
       "        ..\n",
       "45610    2\n",
       "45611    2\n",
       "45612    2\n",
       "45613    1\n",
       "45614    1\n",
       "Name: label, Length: 45615, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c8ae3",
   "metadata": {},
   "source": [
    "Architecture: DDAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b2da3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilledDAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch implementation for Deep Averaging Network for sentiment analysis\n",
    "    Uses Hinton et al.'s 'distillation loss' which compares soft labels from a teacher model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes,\n",
    "                       embedding_dim: int, \n",
    "                       hidden_dim1: int, \n",
    "                       hidden_dim2: int, \n",
    "                       hidden_dim3: int, \n",
    "                       leaky_relu_negative_slope: float, \n",
    "                       dropout_probability: float,\n",
    "                       has_third_hidden_layer: bool,\n",
    "                       has_dropout_on_input: bool\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create the network architecture. \n",
    "        In our sentiment analysis, we have three classes: 0, 1, 2\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.hidden_dim3 = hidden_dim3\n",
    "        self.leaky_relu_negative_slope = leaky_relu_negative_slope\n",
    "        self.dropout_probability = dropout_probability\n",
    "        \n",
    "        self.hidden1 = nn.Linear(self.embedding_dim, self.hidden_dim1)\n",
    "        self.hidden2 = nn.Linear(self.hidden_dim1,self.hidden_dim2)\n",
    "        self.theta = nn.Linear(self.hidden_dim2, self.num_classes)\n",
    "\n",
    "        # Check if hidden3 set to True and adjust theta dimensions accordingly\n",
    "        if has_third_hidden_layer:\n",
    "            self.hidden3 = nn.Linear(self.hidden_dim2,self.hidden_dim3)\n",
    "            self.theta = nn.Linear(self.hidden_dim3, self.num_classes)\n",
    "        \n",
    "        self.log_softmax = nn.Softmax(dim=1) # A dimension along which LogSoftmax will be computed.\n",
    "        self.apply_dropout = nn.Dropout(self.dropout_probability)\n",
    "        self.has_dropout_on_input = has_dropout_on_input\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor containing embedded word vectors.\n",
    "                              Shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Log probability of each class. Shape: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Average the input word embeddings\n",
    "        if self.has_dropout_on_input:\n",
    "            x = self.apply_dropout(x)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Pass through the shared layers\n",
    "        x = self.hidden1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_negative_slope)\n",
    "        x = self.apply_dropout(x)\n",
    "\n",
    "        x = self.hidden2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=self.leaky_relu_negative_slope)\n",
    "        x = self.apply_dropout(x)\n",
    "\n",
    "        # Pass through final layer\n",
    "        x = self.theta(x)\n",
    "\n",
    "        # Apply the LogSoftmax activation function\n",
    "        x = self.log_softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def train_model(self,\n",
    "                    X_train,\n",
    "                    Y_train,\n",
    "                    X_dev,\n",
    "                    Y_dev,\n",
    "                    soft_labels,\n",
    "                    optimizer,\n",
    "                    num_iterations,\n",
    "                    soft_label_weight=0.5,\n",
    "                    loss_fn=nn.CrossEntropyLoss(),\n",
    "                    batch_size=1000,\n",
    "                    check_every=10,\n",
    "                    verbose=False):\n",
    "        \"\"\"\n",
    "        Method to train the model. \n",
    "\n",
    "        soft_labels are only available for the training set. \n",
    "        \"\"\"\n",
    "\n",
    "        # Let the model know that we're in training mode, which is important for dropout\n",
    "        self.train()\n",
    "\n",
    "        loss_history = []\n",
    "        train_accuracy = []\n",
    "        dev_accuracy = []\n",
    "\n",
    "        for t in range(num_iterations):\n",
    "            if batch_size >= X_train.shape[0]: \n",
    "                X_batch = X_train\n",
    "                Y_batch = Y_train\n",
    "                soft_labels_batch = soft_labels\n",
    "            else:\n",
    "                batch_indices = np.random.randint(X_train.shape[0], size=batch_size)\n",
    "                X_batch = X_train[batch_indices]\n",
    "                Y_batch = Y_train[batch_indices]\n",
    "                soft_labels_batch = soft_labels[batch_indices]\n",
    "\n",
    "            # Forward pass \n",
    "            log_probs_batch = self.forward(X_batch)\n",
    "\n",
    "            # Distillation loss (cross entropy loss with hard labels + cross entropy loss with soft labels)\n",
    "            # weighted with soft and hard label\n",
    "            loss = (1 - soft_label_weight) * loss_fn(log_probs_batch, Y_batch) + \\\n",
    "                    soft_label_weight * loss_fn(log_probs_batch, soft_labels_batch)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % check_every == 0:\n",
    "                loss_value = loss.item()\n",
    "                loss_history.append(loss_value)\n",
    "\n",
    "                # Check train accuracy (entire set, not just batch) \n",
    "                train_y_pred = self.predict(X_train)\n",
    "                train_acc = self.accuracy(train_y_pred, Y_train.detach().numpy()) \n",
    "                train_accuracy.append(train_acc)\n",
    "\n",
    "                # Check dev accuracy (entire set, not just batch) \n",
    "                dev_y_pred = self.predict(X_dev)\n",
    "                dev_acc = self.accuracy(dev_y_pred, Y_dev.detach().numpy())\n",
    "                dev_accuracy.append(dev_acc)\n",
    "\n",
    "                if verbose: print(f\"Iteration={t}, Loss={loss_value}\")\n",
    "\n",
    "        return loss_history, train_accuracy, dev_accuracy\n",
    "\n",
    "    \n",
    "    def predict(self, X, proba_mode=False):\n",
    "        \"\"\"\n",
    "        Method to make predictions given a trained model. \n",
    "        \n",
    "        No need to modify this method. \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        log_probs_batch = self.forward(X)\n",
    "\n",
    "        if proba_mode:\n",
    "            return log_probs_batch\n",
    "        else:\n",
    "            # Convert log probabilities to labels\n",
    "            label_batch = proba_to_label(log_probs_batch)\n",
    "            return label_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred: np.ndarray, y_true: np.ndarray) -> float: \n",
    "        \"\"\"\n",
    "        Calculates accuracy. \n",
    "        \"\"\"\n",
    "        return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638daadd",
   "metadata": {},
   "source": [
    "### Grid Search to Tune Hyperparameters\n",
    "Parameters to tune:\n",
    "\n",
    "- learning rate\n",
    "- dropout probability\n",
    "- soft label weight\n",
    "- number of layers (we will try two and three)\n",
    "\n",
    "Notes for developer:\n",
    "\n",
    "- Store the iteration at which the highest accuracy occurs on each model\n",
    "- Store this in a table\n",
    "- Set global pytorch random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec9e6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch random seed\n",
    "torch.manual_seed(4)\n",
    "np.random.seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "419e09ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/s6xvfp8x29j9m9pl6w7jn8hh0000gn/T/ipykernel_21663/2578902843.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(Xmat_train, dtype=torch.float32)\n",
      "/var/folders/2m/s6xvfp8x29j9m9pl6w7jn8hh0000gn/T/ipykernel_21663/2578902843.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_dev = torch.tensor(Xmat_dev, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Load and pre-process data\n",
    "X_train = torch.tensor(Xmat_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "X_dev = torch.tensor(Xmat_dev, dtype=torch.float32)\n",
    "Y_dev = torch.tensor(Y_dev, dtype=torch.long)\n",
    "soft_labels = torch.tensor(Y_soft_train, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110df412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_student(hidden3=False):\n",
    "    '''\n",
    "    Performs a grid search to pick the combination of hyperparameters for the student\n",
    "    Distilled Deep Averaging Network with the best accuracy on the task of sentiment analysis\n",
    "    \n",
    "    Fixed parameters:\n",
    "    - input embedding (pre-trained GloVE embeddings, 50d, vocab of 1.2M)\n",
    "    - loss function\n",
    "    - number of iterations of gradient descent\n",
    "    \n",
    "    Hyper parameters:\n",
    "    - learning rate [1e-5, 1e-4, 1e-3, 1e-2] \n",
    "    - dropout probability [0.0, 0.2, 0.4, 0.6, 0.8] \n",
    "    - soft label weight [0.0, 0.2, 0.5, 0.4, 0.6, 0.8]\n",
    "    - # of layers (2 or 3)\n",
    "\n",
    "    Potential Future Step:\n",
    "    - test performance with dropout on input embeddings\n",
    "    \n",
    "    Returns \n",
    "        1) array of dictionaries containing 'train accuracies', 'dev accuracies', 'best iteration' \n",
    "        (containing the index of the dev accuracies array with the highest value), 'loss history'\n",
    "        2) dictionary containing the combination of hyperparameters with the highest dev accuracy\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    best = {}\n",
    "    overall_best_dev_accuracy = 0\n",
    "    \n",
    "#     # Hyperparameters\n",
    "#     learning_rates = [.0001, .001, .01]\n",
    "#     dropout_probs = [0.0,0.25]\n",
    "#     soft_label_weights = [0.0, 0.2, 0.5, 0.8]\n",
    "#     dropout_input = [False]\n",
    "#     batch_sizes = [500, 1000]\n",
    "#     hidden_dims1 = [64, 128]\n",
    "#     hidden_dims2 = [32, 64]\n",
    "    learning_rates = [.01]\n",
    "    dropout_probs = [0.25]\n",
    "    soft_label_weights = [0.8]\n",
    "    dropout_input = [False]\n",
    "    batch_sizes = [1000]\n",
    "    hidden_dims1 = [128]\n",
    "    hidden_dims2 = [64]\n",
    "    \n",
    "    # Fixed parameters, model architecture\n",
    "    num_classes = 3\n",
    "    embedding_dim = 50\n",
    "    leaky_relu_negative_slope = 0.1\n",
    "    \n",
    "    # Fixed parameters, model training\n",
    "    num_iterations = 10000\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    check_every = 10\n",
    "    verbose = False\n",
    "    \n",
    "    # Grid Search\n",
    "    for learning_rate in learning_rates:\n",
    "        for dropout_prob in dropout_probs:\n",
    "            for soft_label_weight in soft_label_weights:\n",
    "                for is_dropout_input in dropout_input:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        for hidden_dim1 in hidden_dims1:\n",
    "                            for hidden_dim2 in hidden_dims2:\n",
    "\n",
    "                                # Create model\n",
    "                                model = DistilledDAN(num_classes, embedding_dim, hidden_dim1, hidden_dim2, 0,\n",
    "                                                     leaky_relu_negative_slope, dropout_prob, hidden3, is_dropout_input)\n",
    "\n",
    "                                # Step 3: Train the model using the `train_model` method\n",
    "                                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                                loss_history, train_accuracy, dev_accuracy = model.train_model(X_train, Y_train, X_dev, Y_dev, soft_labels, optimizer, num_iterations, soft_label_weight, loss_fn, batch_size, check_every, verbose)\n",
    "\n",
    "                                best_dev_iteration = np.argmax(dev_accuracy)\n",
    "\n",
    "                                hyperparameters = {'learning rate': learning_rate,\n",
    "                                                   'dropout prob': dropout_prob, \n",
    "                                                   'soft label weight': soft_label_weight,\n",
    "                                                   'is_dropout_input': is_dropout_input,\n",
    "                                                   'batch size': batch_size,\n",
    "                                                   'hidden_dim1': hidden_dim1,\n",
    "                                                   'hidden_dim2': hidden_dim2}\n",
    "\n",
    "                                result = {'hyperparameters': hyperparameters,\n",
    "                                          'train accuracy at best dev iter': train_accuracy[best_dev_iteration], \n",
    "                                          'best dev accuracy': dev_accuracy[best_dev_iteration], \n",
    "                                          'best dev iteration': best_dev_iteration}\n",
    "\n",
    "                                # Update best overall dev accuracy to get the best model hyperparameters\n",
    "                                # at end of grid search\n",
    "                                if dev_accuracy[best_dev_iteration] > overall_best_dev_accuracy:\n",
    "                                    overall_best_dev_accuracy = dev_accuracy[best_dev_iteration]\n",
    "                                    best = result\n",
    "                                \n",
    "                                print(result)\n",
    "                                results.append(result)\n",
    "    \n",
    "    return results, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5c6b459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyperparameters': {'learning rate': 0.01, 'dropout prob': 0.25, 'soft label weight': 0.8, 'is_dropout_input': False, 'batch size': 1000, 'hidden_dim1': 128, 'hidden_dim2': 64}, 'train accuracy at best dev iter': 0.6279732544119259, 'best dev accuracy': 0.625, 'best dev iteration': 932}\n",
      "Total time:  241.2146179676056\n"
     ]
    }
   ],
   "source": [
    "# result is a dictionary containing the following keys:\n",
    "#   'train accuracies'\n",
    "#   'dev accuracies'\n",
    "#   'best iteration'\n",
    "#   'loss history'\n",
    "start = time.time()\n",
    "results, best = grid_search_student()\n",
    "end = time.time()\n",
    "total_time = end-start\n",
    "print(\"Total time: \", total_time)\n",
    "\n",
    "\n",
    "with open('grid_search_results.txt', 'w') as results_file:\n",
    "    for elem in results:\n",
    "        results_file.write('%s\\n' % elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e262e",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279cd50a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain accuracies\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev accuracies\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDev Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss history'"
     ]
    }
   ],
   "source": [
    "plt.plot(results[0]['loss history'], label='Loss')\n",
    "plt.plot(results[0]['train accuracies'], label='Train Accuracy')\n",
    "plt.plot(results[0]['dev accuracies'], label='Dev Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32184324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
