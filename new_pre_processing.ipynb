{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f3aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "368f48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5ee47",
   "metadata": {},
   "source": [
    "Generating Soft Labels from the Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16a501f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load training and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    train = pd.read_csv(\"./dataset_twitter/train.csv\")\n",
    "    test = pd.read_csv(\"./dataset_twitter/test.csv\")\n",
    "\n",
    "    Xmat = train[\"text\"]\n",
    "    Y = train[\"label\"]\n",
    "\n",
    "    Xmat_test = test[\"text\"]\n",
    "    Y_test = test[\"label\"]\n",
    "    \n",
    "    return Xmat, Y, Xmat_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d872ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_softlabels(X):\n",
    "    \"\"\"\n",
    "    Citation: Full Classification Example on twitter-roberta-base-sentiment-latest Model card. \n",
    "    Link: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latesthttps://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "    \n",
    "    Runs Twitter-Roberta-Based-Sentiment Model on the Twitter Sentiment Extraction dataset\n",
    "    Returns an array of soft labels (log probabilities)\n",
    "    \"\"\"\n",
    "    # Preprocess text (username and link placeholders)\n",
    "    def preprocess(text):\n",
    "        text = str(text)\n",
    "        new_text = []\n",
    "        for t in text.split(\" \"):\n",
    "            t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "            new_text.append(t)\n",
    "        return \" \".join(new_text)\n",
    "    \n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    config = AutoConfig.from_pretrained(MODEL)\n",
    "    \n",
    "    # Pytorch\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "    # dimensions: [# examples, # classes (3)]\n",
    "    soft_labels = np.zeros((X.shape[0],3))\n",
    "    \n",
    "    for index, row in enumerate(X):\n",
    "        text = preprocess(row)\n",
    "        encoded_input = tokenizer(text, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        soft_labels[index] = scores\n",
    "    \n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "\n",
    "    create_csv_with_labels(soft_labels, \"Y_soft\", \"train.csv\")\n",
    "    return soft_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0c352",
   "metadata": {},
   "source": [
    "Encoding Hard Labels and Adding to Train, Dev, and Test CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "afa097ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels_one_hot(Y):\n",
    "    \"\"\"\n",
    "    For each row in input Y, converts labels 0, 1, and 2 to arrays\n",
    "    that are one-hot encoded\n",
    "    \"\"\"\n",
    "    encoded_Y = np.zeros((Y.shape[0],3))\n",
    "    \n",
    "    for index, row in enumerate(Y):\n",
    "        one_hot_array = np.zeros(3)\n",
    "        one_hot_array[row] = 1\n",
    "        encoded_Y[index] = one_hot_array\n",
    "    \n",
    "    return encoded_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "33fe6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_to_csv(data, header, output_file: str) -> None:\n",
    "    data = {\"Y_hard\": list(data)}\n",
    "    df = pd.DataFrame(data)\n",
    "    file = pd.read_csv(output_file)\n",
    "    out = file.merge(df, how='outer', left_index=True,right_index=True)\n",
    "    with open(output_file, 'w') as f:\n",
    "        out.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e9b03985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_with_labels(labels, header, output_file: str) -> None:\n",
    "    data = {header: list(labels)}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "12777a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Y_hard\n",
      "0  [1, 0, 0]\n",
      "1  [0, 1, 0]\n",
      "2  [0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST for create_csv_with_labels\n",
    "\n",
    "soft = np.array([np.array([.1,.8,.1]), np.array([.2,.7,.1]), np.array([.3,.4,.3])])\n",
    "hard = np.array([np.array([1,0,0]),np.array([0,1,0]),np.array([0,0,1])])\n",
    "create_csv_with_labels(soft, \"Y_soft\", \"train.csv\")\n",
    "add_hard_labels_to_csv(hard, \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c63373",
   "metadata": {},
   "source": [
    "Train-Dev-Test Split of Dataset, Creating CSVs with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "66af869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmat, Y, Xmat_test, Y_test = load_dataset()\n",
    "Xmat_train, Xmat_dev, Y_train, Y_dev = train_test_split(Xmat, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c37a1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) negative 0.6593\n",
      "2) neutral 0.29\n",
      "3) positive 0.0506\n",
      "Total time:  20.433573007583618\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST get_softlabels(X)\n",
    "\n",
    "X = Xmat_train[4000:4200]\n",
    "start = time.time()\n",
    "Xmat_train_soft = get_softlabels(X)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST encode_labels_one_hot\n",
    "\n",
    "Y = Y_train[0:5]\n",
    "print(Y)\n",
    "new_Y = encode_labels_one_hot(Y)\n",
    "print(new_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2d79eabb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) negative 0.9289\n",
      "2) neutral 0.063\n",
      "3) positive 0.0081\n",
      "Total time:  1905.9214930534363\n",
      "                Y_hard\n",
      "0      [0.0, 0.0, 1.0]\n",
      "1      [1.0, 0.0, 0.0]\n",
      "2      [0.0, 0.0, 1.0]\n",
      "3      [0.0, 1.0, 0.0]\n",
      "4      [0.0, 1.0, 0.0]\n",
      "...                ...\n",
      "19231  [0.0, 0.0, 1.0]\n",
      "19232  [0.0, 1.0, 0.0]\n",
      "19233  [0.0, 1.0, 0.0]\n",
      "19234  [1.0, 0.0, 0.0]\n",
      "19235  [1.0, 0.0, 0.0]\n",
      "\n",
      "[19236 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add soft labels to train.csv\n",
    "start = time.time()\n",
    "Xmat_train_soft = get_softlabels(Xmat_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time: \", end - start)\n",
    "\n",
    "# Add hard labels to train.csv\n",
    "Y_train = encode_labels_one_hot(Y_train)\n",
    "add_column_to_csv(Y_train, \"Y_hard\", \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6b74b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Y labels into arrays of size 3 (for each sentiment label output)\n",
    "# Use one-hot encode\n",
    "# Add to CSVs or create CSVs (if not for training dataset)\n",
    "Y_dev = encode_labels_one_hot(Y_dev)\n",
    "Y_test = encode_labels_one_hot(Y_test)\n",
    "Y_train = encode_labels_one_hot(Y_train)\n",
    "\n",
    "add_column_to_csv(Y_train, \"Y_hard\", \"train.csv\")\n",
    "create_csv_with_labels(Y_test, \"Y\", \"test.csv\")\n",
    "create_csv_with_labels(Y_dev, \"Y\", \"dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1d577",
   "metadata": {},
   "source": [
    "Tokenizing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text: str):\n",
    "    '''\n",
    "    Citation: This tokenizer function & regex rule is borrowed from Katie's tokenizer regex demo at:\n",
    "    https://www.cs.williams.edu/~kkeith/teaching/s23/cs375/attach/tokenization_regex_demo.html\n",
    "    This helper function takes a string and returns a list of tokenized strings.\n",
    "    '''\n",
    "    regex = r\"[A-Za-z]+|\\$[\\d\\.]+|\\S+\" \n",
    "    res = nltk.regexp_tokenize(text, regex)\n",
    "    return [i for i in res if i != \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585db0d",
   "metadata": {},
   "source": [
    "Embedding Functions (including Truncation and Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Loads embeddings from embedding file and creates \n",
    "    1) dictionary of embedding words to indices\n",
    "    2) list of embedding indices to words\n",
    "    3) dense word embedding matrix\n",
    "    \"\"\"\n",
    "    embeddings = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "    vocab2indx = dict(embeddings.key_to_index)\n",
    "    idx2vocab = list(embeddings.index_to_key)\n",
    "    embed_array = embeddings.vectors # matrix of dense word embeddings \n",
    "                                     # rows: a word \n",
    "                                     # columns: dimensions (50) of the dense embeddings\n",
    "    return vocab2indx, idx2vocab, embed_array\n",
    "\n",
    "\n",
    "def add_the_embedding(embed_array, vocab2indx): \n",
    "    \"\"\"\n",
    "    Adds \"the\" embedding to the embed_array matrix\n",
    "    \"\"\"\n",
    "    the_embedding = embed_array[vocab2indx[\"the\"]]\n",
    "    out = np.vstack((embed_array, the_embedding))\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_oov(idx2vocab, vocab2indx, embed_array):\n",
    "    \"\"\"\n",
    "    Adds <OOV> token to embedded vocabulary\n",
    "    \"\"\"\n",
    "    print(\"len embed array: \", len(embed_array))\n",
    "    new_oov_entry = len(embed_array)\n",
    "    idx2vocab += [\"<OOV>\"]\n",
    "    vocab2indx[\"<OOV>\"] = new_oov_entry\n",
    "    embed_array_w_oov = add_the_embedding(embed_array, vocab2indx)\n",
    "\n",
    "    return idx2vocab, vocab2indx, embed_array_w_oov\n",
    "\n",
    "\n",
    "def add_pad(idx2vocab, vocab2indx, embed_array):\n",
    "    \"\"\"\n",
    "    Adds <PAD> token to embedded vocabulary\n",
    "    \"\"\"\n",
    "    print(\"len embed array: \", len(embed_array))\n",
    "    new_pad_entry = len(embed_array)\n",
    "    idx2vocab += [\"<PAD>\"]\n",
    "    vocab2indx[\"<PAD>\"] = new_pad_entry\n",
    "    embed_array_w_pad = add_the_embedding(embed_array, vocab2indx)\n",
    "    \n",
    "    return idx2vocab, vocab2indx, embed_array_w_pad\n",
    "\n",
    "\n",
    "def truncate(original_indices_list: list, maximum_length=100) -> list: \n",
    "    \"\"\"\n",
    "    Truncates the original_indices_list to the maximum_length\n",
    "    \"\"\"\n",
    "    return original_indices_list[0:maximum_length]\n",
    "\n",
    "\n",
    "def pad(original_indices_list: list, pad_index: int, maximum_length=100) -> list: \n",
    "    \"\"\"\n",
    "    Given original_indices_list, concatenates the pad_index enough times \n",
    "    to make the list to maximum_length. \n",
    "    \"\"\"\n",
    "    while len(original_indices_list) < maximum_length:\n",
    "        original_indices_list.append(pad_index)\n",
    "        \n",
    "    return original_indices_list\n",
    "\n",
    "\n",
    "def get_padded_oov_embeddings():\n",
    "    \"\"\"\n",
    "    Get embedding array which includes the <PAD> and <OOV> tokens\n",
    "    \"\"\"\n",
    "    vocab2indx, idx2vocab, embed_array = load_embeddings(\"glove.twitter.27B.100d.txt\")\n",
    "    idx2vocab, vocab2indx, embed_array_w_oov = add_oov(idx2vocab, vocab2indx, embed_array)\n",
    "    idx2vocab, vocab2indx, embed_array_w_oov_pad = add_pad(idx2vocab, vocab2indx, embed_array_w_oov)\n",
    "    \n",
    "    return embed_array_w_oov_pad, vocab2indx, idx2vocab\n",
    "\n",
    "def create_word_indices(tokens, vocab2indx): \n",
    "    \"\"\"\n",
    "    For each example, translate each token into its corresponding index from vocab2indx\n",
    "    \n",
    "    Replace words not in the vocabulary with the symbol \"<OOV>\" \n",
    "        which stands for 'out of vocabulary'\n",
    "        \n",
    "    Arguments: \n",
    "       - tokens (List[str]): list of strings of tokens \n",
    "       - vocab2indx (dict): each vocabulary word as strings and its corresponding int index \n",
    "                           for the embeddings \n",
    "                           \n",
    "    Returns: \n",
    "        - (List[int]): list of integers\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in vocab2indx:\n",
    "            token = \"<OOV>\"\n",
    "        indices.append(vocab2indx[token])\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def convert_X(Xmat):\n",
    "    MAXIMUM_LENGTH = 128\n",
    "    embeddings, vocab2indx, idx2vocab = get_padded_oov_embeddings()\n",
    "    \n",
    "    X_list = []\n",
    "    for one_train_example in Xmat:\n",
    "        one_train_example = tokenizer(one_train_example)\n",
    "        one_train_indices = create_word_indices(one_train_example, vocab2indx)\n",
    "        one_train_indices = truncate(one_train_indices, maximum_length=MAXIMUM_LENGTH)\n",
    "        one_train_indices = pad(one_train_indices, len(vocab2indx)-1, maximum_length=MAXIMUM_LENGTH)\n",
    "        \n",
    "        one_train_example_embeddings = [] # A list of token embeddings\n",
    "        \n",
    "        for index in one_train_indices:\n",
    "            one_train_example_embeddings.append(embeddings[index])\n",
    "        \n",
    "        X_list.append(one_train_example_embeddings)\n",
    "        \n",
    "#     X = torch.FloatTensor(X_list)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4addbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTEGRATION TEST, get_padded_oov_embeddings\n",
    "\n",
    "embeddings, vocab2indx, idx2vocab = get_padded_oov_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4709013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to train, dev, and test CSVs\n",
    "\n",
    "X_train_embedded = convert_X(Xmat_train)\n",
    "X_dev_embedded = convert_X(Xmat_dev)\n",
    "X_test_embedded = convert_X(Xmat_test)\n",
    "\n",
    "add_column_to_csv(X_train_embedded, \"Embedding\", \"train.csv\")\n",
    "add_column_to_csv(X_dev_embedded, \"Embedding\", \"dev.csv\")\n",
    "add_column_to_csv(X_test_embedded, \"Embedding\", \"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
