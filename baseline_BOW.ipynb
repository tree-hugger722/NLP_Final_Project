{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76e38db",
   "metadata": {},
   "source": [
    "### Run Baseline: Logistic Regression with BoW Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78935471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3dbaefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"\n",
    "    Load training and dev sets\n",
    "    \"\"\"\n",
    "\n",
    "    train = pd.read_csv(\"./new_dataset/train_preprocessed.csv\")\n",
    "    dev = pd.read_csv(\"./new_dataset/val_preprocessed.csv\")\n",
    "    \n",
    "    return train, dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f22a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc24b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text: str):\n",
    "    '''\n",
    "    NLTK Tweet Tokenizer -- removes handles\n",
    "\n",
    "    @param text        string tweet\n",
    "    @ret tokens        list of tokens\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer(strip_handles=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd2ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have train and test sets\n",
    "Xmat_train = train[\"text\"]\n",
    "Y_train = train[\"Y_hard\"]\n",
    "\n",
    "Xmat_dev = dev[\"text\"]\n",
    "Y_dev = dev[\"Y_hard\"]\n",
    "\n",
    "# Handle missing values\n",
    "Xmat_train.fillna('', inplace=True)\n",
    "Xmat_dev.fillna('', inplace=True)\n",
    "\n",
    "# Create a Bag of Words representation using the training data *only*\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer)\n",
    "X_train_bow = vectorizer.fit_transform(Xmat_train)\n",
    "X_dev_bow = vectorizer.transform(Xmat_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e26cc",
   "metadata": {},
   "source": [
    "Run model on train and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76785fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9095472980379261\n",
      "Dev accuracy: 0.6775\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "baseline_bow = LogisticRegression(max_iter=2000, multi_class='auto', solver='lbfgs')\n",
    "baseline_bow.fit(X_train_bow, Y_train)\n",
    "\n",
    "# Make predictions\n",
    "Y_pred_train = baseline_bow.predict(X_train_bow)\n",
    "Y_pred_dev = baseline_bow.predict(X_dev_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = accuracy_score(Y_train, Y_pred_train)\n",
    "dev_accuracy = accuracy_score(Y_dev, Y_pred_dev)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}\")\n",
    "print(f\"Dev accuracy: {dev_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9462da80",
   "metadata": {},
   "source": [
    "Run on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9776b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./new_dataset/test_preprocessed.csv\")\n",
    "\n",
    "Xmat_test = test[\"text\"]\n",
    "Y_test = test[\"Y_hard\"]\n",
    "\n",
    "X_test_bow = vectorizer.transform(Xmat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "358d16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5902800390752198\n"
     ]
    }
   ],
   "source": [
    "Y_pred_test = baseline_bow.predict(X_test_bow)\n",
    "test_accuracy = accuracy_score(Y_test, Y_pred_test)\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ef1150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.0, 1.0, 0.0]'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c9c28f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.0, 1.0, 0.0]'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f200cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = [0, 0, 0]\n",
    "wrong = [0, 0, 0]\n",
    "for y, y_pred in zip(Y_test, Y_pred_test):\n",
    "    y = eval(y)\n",
    "    y_pred = eval(y_pred)\n",
    "    total[np.argmax(y)] += 1\n",
    "    if np.argmax(y) != np.argmax(y_pred):\n",
    "        # if the prediction is wrong\n",
    "        wrong[np.argmax(y)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81e52b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3972, 5937, 2375]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c36a04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_error = []\n",
    "\n",
    "for i in range(len(total)):\n",
    "    percent_error.append(wrong[i]/total[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df126d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5599194360523666, 0.3084049183089102, 0.41178947368421054]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cs375] *",
   "language": "python",
   "name": "conda-env-.conda-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
